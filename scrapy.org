* 常用的命令行工具
   分两种，一种是只能在项目内部使用的，另一种是也可在项目外使用的。
   
   创建新的项目:
   $ scrapy startproject 项目名
   
   在项目内运行爬虫
   $ scrapy crawl 爬虫名
   
   在项目内显示当前项目中所有爬虫的名称
   $ scrapy list
   
   在项目内编辑爬虫，默认的编辑器通过 shell 的 EDITOR 环境变量来设置
   $ scrapy edit 爬虫名
   
   在项目内显示可用的爬虫的类型
   $ scrapy genspider -l
   在项目内显示爬虫模板
   $ scrapy genspider -d 爬虫类型
   在项目内利用爬虫模板编辑新的爬虫
   $ scrapy genspider -t 爬虫类型 爬虫name 爬虫allowed_domains
   
   在浏览器中查看 scrapy 看到的网页的数据，可能 scrapy 看到的网页的数据和人看到
   的有不同，通过这个方法可以检测下
   $ scrapy view 网页url
   
   starts the Scrapy shell for the given URL (if given) or empty if not URL is
   given，可通过这种方法来交互查看爬虫中某些语句的执行效果，如 XPATH
   $ scrapy shell 网页url
   
   Run a spider self-contained in a Python file, without having to create a
   project，在项目外执行
   $ scrapy runspider <spider_file.py>
* Items
*** goal
    The main goal in scraping is to extract structured data from unstructured
    sources, typically, web pages. Scrapy provides teh /Item/ class for this
    purpose.
    /Item/ objects are simple containers used to collect the scraped data. They
    provide a dictionary-like API with a convenient syntax for declaring their
    available fields.
*** Fields
    /Field/ objects are used to specify metadata for each field.
    The main goal of /Field/ objects is to provide a way to define all field
    metadata in one place. Typically, those components whose behaviour depends
    on each field use certain field keys to configure that behaviour. You must
    refer to their documentation to see wich metadata keys are used by each
    component.
* Spiders
** goal
    Spiders are classes which define how a certain site (or domain) will be
    scraped, including how to crawl the site and how to extracted items from
    their pages. In other words, Spiders are the place where you define the
    curstom behaviour for crawling and parsing pages for a particular site.
** 可用的爬虫
*** scrapy.spider.BaseSpider
     This is the simplest spider, and the one from which every other spider must
     inherit from (either the ones that come bundled with Scrapy, or the ones
     that you write yourself). It doesn't provide any special functionality. It
     just requests the given _start_urls/start_requests_, and calls the spider's
     method _parse_ for each of the resulting responses.
     
     + *name*
       The spider name is how the spider is located (and instantiated) by
       Scrapy, so it must be unique.
       It is recommended to name your spiders after the domain that their crawl.
     + *allowed_domains*
       An optional list of strings containing domains that this spider is
       allowed to crawl. Requests for URLs not belonging to the domain names
       specified in this list won't be followed if _OffsiteMiddleware_ is
       enabled.
	   若不设置这个参数，则允许所有的域。
     + *start_urls*
       A list.
     + *start_requests()*
       _认真理解这个方法。_
       This method must return an iterable with the first Requests to crawl for
       this spider.
       This is the method called by Scrapy when the spider is opened for
       scraping when no particular URLs are specified. If particular URLs are
       specified, the _make_requests_from_url()_ is used instead to create the
       Requests. This method is also called only once from Scrapy, so it's safe
       to implement it as a generator.
       The default implementation uses _make_requests_from_url()_ to generate
       Requests for each url in _start_urls_ .
       If you want to change the Requests used to start scraping a domain, this
       is the method to override. For example, if you need to start by logging
       in using a POST request, you could do:
       
       def start_requests(self):
           return [FormRequest("http://www.example.com/login",
	                        formdata={'user': 'john', 'pass': 'secret'},
				callback=self.logged_in)]
       def logged_in(self, response):
           # here you would extract links to follow and return Requests for
           # each of them ,with another callback
           pass
     + *make_requests_from_url(url)*
       A method that receives a URL and returns a /Request/ object (or a list of
       /Request/ objects) to scrape. This method is used to construct the
       initial requests in the _start_requests()_ method, and is typically used
       to convert urls to requests.
       Unless overridden, this method returns Requests with the _parse()_ method
       as their callback function, and with dont_filter parameter enabled.

     + *parse(response)*
       This is the default callback used by Scrapy to process downloaded
       responses, when their requests don't specify a callback.
       The _parse_ method is in charge of processing the response and returning
       scraped data and/or more URLs to follow. Other Requests callbacks have
       the same requirement as the /BaseSpider/ class.
       This method, as well as any other Request callback, must return an
       iterable of /Item/ objects.

     + *log(message [, level, component])*
       Log a message using the _scrapy.log.msg()_ function, automatically
       populating the spider argument with the /name/ of this spider.

*** scrapy.contrib.spiders.CrawlSpider
     This is the most commonly used spider for crawling regular websites, as it
     provides a convenient mechanism for following links by defining a set of
     rules. 
     Apart from teh attributes inherited from /BaseSpider/ (that you must
     specify), this class supports a new attribute:
     
     + *rules*
       Which is a list of one (or more) /Rule/ objects. Each /Rule/ defines a
       certain behavior for crawling the site. If multiple rules match the same
       link, the first one will be used, according to the order they're defined
       in this attribute.

     + scrapy.contrib.spiders.Rule(link_extractor, callback=None,
       cb_kwargs=None,follow=None, process_links=None, process_request=None)
       - *link_extractor*
	 It's a /Link Extractor/ object which defines how links will be
         extracted from each crawled page.
       - *callback*
	 It's a callable or a string (in which case a method from the spider
         object with that name will be used) to be called for each link
         extracted with the specified /link_extractor/. _This callback receives a_
         _response as its first argument and must return a list containing /Item/_
         _and/or /Request/ objects (or any subclass of them)._ 也可以返回一个
         Item 对象。
	 
	 When writing crawl spider rules, avoid using /parse/ as callback, since
         the _CrawlSpider_ uses the /parse/ method itself to implement its
         logic. So if you override the /parse/ method, the crawl spider will no
         longer work.
       - *cb_kwargs*
	 It's a dict containing the keyword arguments to be passed to the
         callback function.
       - *follow*
	 It's a boolean which specifies if links should be followed from each
         response extracted with this rule. If _callback_ is None *follow*
         defaults to True, otherwise it defaults to False.
       - *process_links*
	 It's a callable, or a string (in which case a method from the spider
         object with that name will be used) which will be called for each list
         of links extracted from each response using the specified
         *link_extractor*. This is mainly used for filtering purposes.
	 It must return a request or None (to filter out the request).
*** scrapy.contrib.spiders.XMLFeedSpider
     XMLFeedSpider is designed for parsing XML feed by iterating through them by
     a certain node name. The iterator can be chosen from: /iternodes, xml/
     /html/ . It's recommended to use the /iternodes/ iterator for performance
     reasons, since the /xml/ and /html/ iterators generate the whole DOM at
     once in order to parse it. However, using /html/ as the iterator may be
     useful when parsing XML with bad markup.
     
     + *iterator*
       A string which defines the iterator to sue. It can be either:
       - *iternodes* --- a fast iterator based on regular expressions
       - *html* --- an iterator which uses HtmlXPathSelector. Keep in mind this
         uses DOM parsing and must load all DOM in memory which could be a
         problem for big feeds
       - *xml* --- an iterator which uses XmlXPathSelector. Keep in mind this
         uses DOM parsing and must load all DOM in memory which could be a
         problem for big feeds
       
       It defaults to *iternodes*
     + *itertag*
       A string with the name of the node (or element) to iterate in.
     + *namespace*
       A list of _(prefix, url)_ tuples which define the namespaces available in
       that document that will be processed with this spider. The _prefix_ and
       _url_ will be used to automatically register namespaces using the
       _register_namespace()_ method.
       You can specify nodes with namespaces in the _itertag_ attribute.
     + *adapt_response(response)*
       A method that receives the response as soon as it arrives from the spider
       middleware, before the spider starts parsing it. It can be used to modify
       the resp body before parsing it. This method receives a response and also
       returns a response (it could be the same or another one).
     + *parse_node(response, selector)*
       This method is called for the nodes matching the provided tag name
       (/itertag/). Receives the response and an XPathSelector for each
       node. Overriding this method is mandatory. Otherwise, your spider won't
       work. This method must return either a _Item__ object, a _Request_
       object, or an iterable containing any of them.
     + *process_results(response, results)*
       This method is called for each result (item or request) returned by the
       spider, and it's intended to perform any last time processing required
       before returning the results to the framework core, for example setting
       the item IDs. It receives a list of results and the response which
       originated those results. It must return a list of results (Items or
       Requests).
* 使用
** 初始化
   在爬虫根目录下执行
   $ scrapy list
   会执行爬虫里类中的方法之上的代码，检查爬虫的正确性。
   比如在爬虫的类的方法前创建文件，则执行以上命令后会执行创建文件的操作。
* Downloader
** 返回 Request()
   若从 Downloader 返回的是 Request(), 则 scrapy engine 直接把该 Request() 返回
   给 scheduler，而不是传给 spider 使用 callback 进行处理。
* HtmlXPathSelector 对有些网页不能正确解析
  参见
  http://stackoverflow.com/questions/12084033/scrapy-couldnt-parse-some-html-file-correctly

  有时通过 HtmlXPathSelector 解析网页时，即使网页中有某个 tag，但通过 xpath 却获
  得不到该 tag。这是因为 HtmlXPathSelector 默认使用 lxml 作为解析后端，而

  lxml determines the encoding of a document by looking at the charset
  specified in the meta tag of the header. Thus, there is an encoding type
  mismatch between what lxml and scrapy perceive.

  有两种解决方案，一种是出现问题时使用 BeautifulSoup 来解析，另一种是在处理页面
  前先探测页面的编码方式，若不是 utf-8，则转换为 utf-8 后再进行其他的操作，如下

  from scrapy.selector import HtmlXPathSelector
  from scrapy.spider import BaseSpider
  import chardet

  class TestSpider(BaseSpider):
      name = "Test"

      start_urls = [
        "http://detail.zol.com.cn/series/268/10227_1.html"
                 ]

      def parse(self, response):

          encoding = chardet.detect(response.body)['encoding']
          if encoding != 'utf-8':
              response.body = response.body.decode(encoding, 'replace').encode('utf-8')

          hxs = HtmlXPathSelector(response)
          data = hxs.select("//div[@id='param-more']").extract()
          print data
* Scrapy 下载图片
  通过 scrapy 手册参考关于下载图片的中间件的设置方法，在爬虫中要注意的是，返回
  Item 时，有且只有一个 key 为 'image_urls' 的 Item，这意味着该 Item 的 value 是
  一个 list，其中包含所有的图片的 url.
  以下是一个完整的参考实例:

  在 items.py 中添加如下语句(可扩展其中的项)：
  
  class ImageItem(Item):
      image_urls = Field()
      images = Field()

  在 settings.py 中添加如下语句:
  
  ITEM_PIPELINES = [
    'scrapy.contrib.pipeline.images.ImagesPipeline'
    ]

  IMAGES_STORE = "images"		# 存储图片的位置

  一个完整的 spider (注意图片的 url 一定要完整，在本例中，获得的图片 url 是完整
  的，但为了通用，采用了 get_base_url() 和 urljoin_rfc() 方法):

  from scrapy.spider import BaseSpider
  from scrapy.selector import HtmlXPathSelector
  from imagespider.items import ImageItem
  from scrapy.utils.response import get_base_url
  from scrapy.utils.url import urljoin_rfc
  import sys, os, time

  reload(sys)
  sys.setdefaultencoding('utf-8')

  class ImageSpider(BaseSpider):
    
      name = 'imagespider'
      start_urls = [
          'http://www.douban.com',
          ]

    
      def __init__(self):
          if not os.path.isdir("data"):
              os.mkdir("data")
          self.fn_log = "data/log.txt"
          self.fp_log = open(self.fn_log, 'ab+')

      def parse(self, response):
          hxs = HtmlXPathSelector(response)
          current_url = response.url
          base_url = get_base_url(response)

          data_log = "%s\n%s\n%s\n\n" % (time.asctime(), current_url, base_url)
          self.fp_log.write(data_log)

          items = []
          xpath_image = "//img/@data-src"
          item = ImageItem()
          item['image_urls'] = [urljoin_rfc(base_url, ul) for ul in hxs.select(xpath_image).extract()]
          # item['image_urls'] = [ul for ul in hxs.select(xpath_image).extract()]
          items.append(item)
            
          return items

  默认的图片的名称是它的 url 的 SHA1 hash 值，若想个性化图片的名字，可扩展
  ImagesPipeline 中间件。
  如爬虫的工程名为 imagespider，则在 imagespider/imagespider/ 目录下创建名为
  ImageMiddleware.py 的文件，在其中添加如下语句:

  from scrapy.contrib.pipeline.images import ImagesPipeline


  class MyImagesPipeline(ImagesPipeline):

    #Name download version
    def image_key(self, url):
        image_guid = url.split('/')[-1][:-4]
        return 'full/%s.jpg' % (image_guid)

   #Name thumbnail version
    def thumb_key(self, url, thumb_id):
        image_guid = thumb_id + url.split('/')[-1]
        return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)

  (可看 ImagesPipeline 的源码来重写其中的相关函数, 若想更个性化，则需要修改这两
  个函数)
  然后在 settings.py 中添加如下语句:

  ITEM_PIPELINES = [
    # 'scrapy.contrib.pipeline.images.ImagesPipeline'
    'imagespider.ImageMiddleware.MyImagesPipeline'
    ]

  IMAGES_STORE = "images"

  即可。
* 常见的问题
** 提示找不到 items 模块
    这可能是因为在 spiders/ 目录下的爬虫的文件名和项目名相同造成的，导致 python
    执行时从爬虫文件所在的目录开始查找模块。
** 抓取 Retry 时提示 scapy Connection to the other side was lost in a non-clean fashion: Connection lost
   网上的解释是:
   He wrote a test-case (connection breaker) and compared the log of broken
   connection and a cleanly closed one, noticing that former gets connectionLost
   event before removing the session, while latter gets it as a result, and
   exploited this difference by introducing a "active_close" flag to session and
   a simple session re-creation sequence in removeMe method.
   参见: http://code.google.com/p/pyicqt/issues/detail?id=5
   
   经过查看我的代码，发现的问题是，我的 pipelines.py 文件中的函数写错了，造成这
   种问题，修改后就正常了。
** BaseSpider 没有 start_urls 时的情况
   结合 BaseSpider 源码和文档，通过 start_requests() 方法可以解决这个问题。
   在 start_requests() 中可以有三种方法。
   一种是模仿 BaseSpider 中的方法，返回 FormRequest() 的对象列表，通过这种方法可
   以指定 callback 函数，默认是调用 parse() 方法作为回调函数。
   第二种是模仿 BaseSpider 源码中的 start_requests() 的实现，通过调用
   make_requests_from_url() 方法，我觉得使用这种方法只能默认使用 parse() 作为回
   调函数（这种想法得需要验证）.
   第三种方法是用 Request() 对象，在 start_requests() 中对于每个 url，使用
   yield Request()
   来做。
   有个问题时，当没有 response 时，似乎不会调用回调函数。

** 递归函数
   可通过 Request() 中的 callback 来调用自身处理响应。但我发现，如果请求的 url
   相同，则不会执行回调函数。
** scrapy shell 无法获得子页的数据
   出现这种情况时，用 scrapy shell 抓取该网站的首页，然后调用 fetch() 方法来获取
   想要的网页的数据。
** 提高抓取效率的几个设置
   可修改 DOWNLOAD_TIMEOUT 和 CONCURRENT_REQUESTS 来提高抓取效率，方法为：
   在爬虫中添加
   
   from scrapy.conf import settings

   settings.overrides['DOWNLOAD_TIMEOUT'] = 5 (默认是 180)
   settings.overrides['CONCURRENT_REQUESTS'] = 200 (默认是 16)
   settings.overrides['CONCURRENT_REQUESTS_PER_DOMAIN'] = 100 (默认是 8)

   在默认的设置中， CONCURRENT_REQUESTS 是 CONCURRENT_REQUESTS_PER_DOMAIN 的 2
   倍，现在还不清楚为什么前者必须比后者大才能起到效果。
   在设置中 CONCURRENT_REQUESTS 最好是 CONCURRENT_REQUESTS_PER_DOMAIN 的 2 倍以
   上。
** 使用 Http 代理
   假如爬虫工程名为 commentid，在 commentid 目录下，建立名为 httpproxy.py 文件，
   在其中写入:
   
   class ProxyMiddleware(object):
       def process_request(self, request, spider):
	       request.meta['proxy'] = 'http://代理ip:代理port'

   然后在 settings.py 中添加如下语句:
   
   DOWNLOADER_MIDDLEWARES = {
    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware':110,
    'commentid.middlewares.ProxyMiddleware':100,
    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware':None,
    }

    尤其要注意最后一条语句，如果不加，可能会返回 "Not a gzipped file" 的错误。
** 抓取京东数据时可能会出现的比较严重的问题
   有时向京东发送请求后返回 error 页面，但该 url 其实是有正常数据的。在处理京东
   的数据时要注意这点儿，记住加条 url 的判断。
** 输入中文
   在文件的第一行输入:
   #coding: utf-8

   这样之后，比如想判断某个中文词组是否在一个 list 中，可直接通过如下形式来实现，
   不要转编码:

   chinese_list = ['测试', '继续', '再次']
   if test_chiese in chinese_list:
       print 'yes'
   else:
       print 'no'
** HtmlXPathSelector 不能正确解析页面
   有时会出现这样的情况，网页中包含某个 tag，但通过 HtmlXPathSelector 却选择不到
   该 tag。
   出现这种情况的原因是，HtmlXPathSelector 默认使用 lxml 作为后端对页面进行解析，
   而当页面结构不规范或存在非法字符时，lxml 对这类页面不能正常解析。lxml 默认使
   用 utf-8 作为编码，而且根据网页中的 meta encoding 来获取网页的编码。GB2312 中
   的有些编码不能被 utf-8 正确识别，因此可能会造成一些错误。
   解决方法参见一级标题中的解决方法。
** user-agent
   默认的 user-agent 和 request header 可通过如下方式获得:
   
   from scrapy.conf import settings
   settings.get('USER_AGENT')
   settings.get('DEFAULT_REQUESTS_HEADERS')
   
   还可通过 settings.py 文件查看 user-agent。发送请求时传递的 user-agent 就是传
   递这个文件中的 user-agent.
   若想设置不同的 user-agent 来欺骗目标网站，可通过在 Request() 对象中添加
   headers 参数来实现。方法为，在构造 Request() 对象时，如下:

   user_header = {'user-agent': 自己设定的值}
   yield Request(url, callback=回调函数, headers=user_header)

   若想在回调函数中查看设定的 headers, 可通过 response.request.headers 来查看，
   它是 dict 类型。
   
   有个网站是
   http://www.useragentstring.com/pages/useragentstring.php
   通过它可以获得很多 user-agent。
** 设置抓取时间间隔
   可通过如下方式在爬虫中修改:

   from scrapy.conf import settings

   settings.overrides['DOWNLOAD_DELAY'] = time_delay  # 请求之间的间隔时间(以秒作单位)
   
   默认情况下请求之间的时间间隔是在 [0.5*time_dealy, 1.5*time_dealy] 之间，若想
   指定间隔时间，可加上

   settings.overrides['RANDOMIZE_DOWNLOAD_DELAY'] = False # 默认是 True
** 使用 CrawlSpider 时提示没有 _rules 属性和 callback 错误
   在 CrawlSpider 的子类中，若定义了 __init__() 方法，则需要显示初始化
   CrawlSpider.__init__(self) 。而且还要注意，还不能使用 super() 来初始化，会有
   问题。
   在 Rule() 中设置 callback 时，不能使用 self，得用 "" 包围 callback 函数。
** 使用 BaseSpider 时在定义的 __init__() 中初始化了 BaseSpider 的 __init__()
   若在 BaseSpider 的子类中定义了 __init__() 方法，则最好不要再显示调用
   BaseSpider.__init__() 方法，因为这时可能会提示如下类型的信息：

   UNFORMATTABLE OBJECT WRITTEN TO LOG with fmt '[%(system)s] %(text)s\n',
   MESSAGE LOST

** 在 Request() 中设置 callback
   假如在类中定义的 callback 函数名是 "test_parse"，在 Request() 中设置 callback
   时要用如下形式:

   callback=self.test_parse

   而不能使用如下的形式:

   callback="test_parse"
** 中断爬虫后再运行
   假设爬虫名是 somespider，则可运行
   
   $ scrapy crawl somespider -s JOBDIR=crawls/somespider-1

   把当前爬虫的状态保存在 crawls/somespider-1 目录下，中断爬虫后，再通过相同的命
   令恢复爬虫的运行。

   *NOTE:*
   我觉得这种方法不太安全，试了两种不同类型的爬虫，都会出现再次执行上述命令后不
   能正常运行爬虫，爬虫自动结束。
   建议不要轻易使用这条命令。
** 不建立工程使用爬虫
   假设爬虫所在的文件名为 test.py，则可按如下方式运行:

   $ scrapy runspider test.py
** 发送 url 后等待超长时间仍无法获得内容响应
   可能的原因有很多中，如请求头部的信息、cookie 等，还可能是需要改变 url 的格式
   再发送请求。要对比下发送的 url 和通过浏览器获得的 url 的区别。
* 源码剖析
** scrapy/spider.py
   定义了 BaseSpider 类，它继承 obejct_ref 类。object_ref 类的作用是记录当前类的
   对象的引用。?????? (对 object_ref 类还不太了解，暂时知道这么多)
   BaseSpider 类中主要定义了发送 Request 对象的方法、设置爬虫、设置 log。默认的
   log 级别是 log.DEBUG。
   BaseSpider 会自动从 start_urls 中发送 Request 对象，通过 start_requests() 方
   法实现，而该方法调用 make_requests_from_url() 方法来发送 Request 对象。若根据
   需求需要手动建立 Request 对象，可不定义 start_urls 变量，而是重新定义
   start_requests() 方法，可在 Request 对象中指定 callback，而使用默认的
   parse() 方法来分析 response.
   该类中只对 parse() 方法做了返回 NoImplementedError 异常的处理，若需要使用该默
   认方法，需要使用者在subclass 该类时重新定义该方法。
   BaseSpider 类没有定义默认的处理 response 的方法，主要做的是发送 Request、设置
   log 和爬虫名等。它是所有爬虫的基类，所有的爬虫必须是该类的 subclass.
** scrapy/contrib/spiders/crawl.py (需要着重理解)
   有两个类，即 Rule 和 CrawlSpider，Rule 类是为 CrawlSpider 服务的。

   CrawlSpider 类的一个特性是可以自动分析 response 中的所有链接，并自动对分析出
   的链接进行抓取，然后继续这个过程。Rule 类定义了抽取 response 中链接的方法、规
   则、对链接的处理、是否继续爬链等。

   Rule 类中需要注意的主要参数有:
   + link_extractor
	 定义了抽取链接的方法。
   + callback
	 定义了处理对爬取到的链接的处理方法
   + follow
	 定义了是否继续爬链。默认为 None.
	 若没设置 follow, 则当 callback 设置时，follow 默认为 False，否则为 True.
   + process_links
	 定义了对分析出的链接的处理方法，如修改格式等
   + process_requests (???)
	 作用暂时不清楚，默认的作用是返回参数。

   CrawlSpider 返回 item 对象或 Request 对象。在这个类中，大量使用了 yield
   + _requests_to_follow()
     处理爬链的方法是 _requests_to_follow()，在这个方法中会遍历 rules 变量中定义的
     Rule 对象，处理每个 Rule 时，会有对符合规则的链接的去重处理，是通过使用 set
     类型来实现的。在这个过程中，可对抓取到的链接进行一些辅助性的工作，通过 Rule
     中的 process_links() 方法来实现。
   + _parse_response()
     _parse_response() 方法对返回的 response 进行处理，要么返回 item 对象，要么返
     回 Request 对象。其中有两部分处理，第一部分是通过 callback 对 response 进行处
     理，第二部分是对是否进行爬链的处理。

   TODO:
   + 需要再理解的方法:
	 - parse()
	   我不理解 CrawlSpider 用这个方法是做什么的。
	 - _parse_response()
	 - _requests_to_follow()
	   不理解 CrawlSpider 是如何代用该方法的。
	 - _response_downloaded()
	   不理解该方法的作用。

** scrapy.selector
*** 综述
	默认是使用 lxml 作为后端，但可选择 libxml2。参见源码
	scrapy/selector/__init__.py
	即 scrapy 中的 selector 默认是调用 lxml 中的方法。
	
	从 scrapy/selector/lxmlsel.py 中可以看出，HtmlXPathSelector 和
	XmlXPathSelector 都是基于 XPathSelector 类。
	XPathSelector 默认使用的分析方法是 lxml 中的 etree.HTMLParser,默认的输出格式
	是 html.
	HtmlXPathSelector 默认使用的分析方法是 lxml 中的 etree.HTMLParser，默认的输
	出格式是 html.
	XmlXPathSelector 默认使用的分析方法是 lxml 中的 etree.XMLParser，默认的输出
	格式是 xml.

	在 XPathSelector 类中，最重要的是 select() 和 extract() 方法。
	select() 在内部会把选择的结果转换为包含节点的 list 类型。从这点儿要注意，使用
	HtmlXPathSelector 的 select() 方法时，返回的都是 list 类型的数据。
	extract() 会把结果使用 etree.tostring() 的方法返回。按照 etree.tostring() 的
	方法，返回的应该是 str 类型的，但实际使用时发现返回的是 list 类型的数据。
** scrapy.http
*** 综述
	要使用与 HTTP 相关的类，直接从 scrapy.http 中调用，提供了 8 个类，一个是
	Headers，3 个请求相关的类，4 个响应相关的类。 
*** Headers 类
	它应该处理的是请求头部。
	------目前没发现它的用处------
*** 请求相关的类
	我常用的是 Request() 类，在初始化时常用的是设置 callback、传递给 Response()
	的参数 meta、请求头部 headers。其中，headers 是 Headers 类的实例，可使用
	Headers 中的属性。
	
	官方文档上说 FormRequest() 比 Request() 更方便，可能是我对函数中的 *args、
	**kwargs 不太会用，因此几乎不用它。

	暂时还没用到 XmlRpcRequest() 类，因为还对 xml-rpc 有深入理解。
*** 响应相关的类
	Response() 类中比较有用的是 response.url、response.status、response.headers、
	response.request、response.body
	其中，response.url 指响应的 url，不一定等于 response.request.url.因为可能发
	     生了跳转，
	     response.status 指响应的 HTTP 状态码，
		 response.headers 指响应的头部(dict 类型)，
		 response.body 可获得响应的 body 部分，
		 response.request 是 Request() 对象，可通过它获得与请求相关的数据，如请
		 求的头部，通过 response.request.headers (dict类型)，通过该头部可验证是
		 否成功使用代理、设置 user-agent 等信息。在 DOWNLOADER_MIDDLEWARES 中不
		 能使用该属性。
	可通过 response.meta[] 的方式访问 Request() 中的参数 meta (dict 类型),其实是
	通过 response.request.meta[] 的方式访问的。
	
	TextResponse 类基于 Response 类，加入了编码处理和发现的操作。

	HtmlResponse 类和 XmlResponse 类是 TextResponse 类的子类，但没做任何处理。
** 问题
*** 模块重命名后的名字相同
	在 scrapy/selector/dummysel.py 中为何在 import 中重命名一个模块时用相同的名
	字?

	from .list import XPathSelectorList as XPathSelectorList

* NOTE
** yield
   scrapy 是通过 yield 发送请求的，但要注意它有个 '副作用'，就是会记录当前程序的
   状态，这样可能会影响再次发送请求时，传入的某些值，如 meta 中的某些数据.这种情
   况已经遇到过一次.
* 最佳实践
  + 在每个处理方法中，若有新的请求需要发送，则把这些 Request 对象集中到一个 list
    中，在处理方法的最后集中发送
  + 要明白每个方法中都有哪些类别的新请求需要发送，写在注释中
  + 说明性的注释用 """""" 表示，debug 或可删除的代码用 # 注释表示
