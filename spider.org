* spider trap
  网站可能为了不让或限制爬虫爬资源，会设置 spider trap，常见的情况是让爬虫经过爬
  链后在一个循环链中不停地爬。
  在写爬虫时，要格外注意出现爬链死循环的情况，即爬链时出现死循环的情况。
  在我写过的爬虫中，如抓取亚马逊的 "商品id:评论id" 的爬虫，在爬"下一页"的链接过
  程中，出现了死循环的情况。因为那时我只找对应的 a[@href] 的最后一个，没有分析它
  是否对应"下一页"。
* 网页中的 mainFrame 和 currentFrame
  我觉得，网页中可以嵌入网页，因此造成这两个概念，在 QtWebKit 和 PhantomJS 中可
  以看到。最初的 '主网页' 是 mainFrame，若当前置于嵌入的网页位置时，就处于
  currentFrame 下。

  在 html 文件中，通过 <frameset> 标签来控制。
* 模拟登陆 '知乎'
  分析 http://www.zhihu.com/#signin 的登陆 form 可知，post 的数据提交给了
  http://www.zhihu.com/login ，这样就清楚向那个 url post 数据了.
  以下是 python 代码:

  import requests

  s = requests.session()
  login_data = {'email': '你的邮箱', 'password': '你的密码', }

  # post 数据
  s.post('http://www.zhihu.com/login', login_data)

  # 验证是否登陆成功，抓取 知乎 首页看看内容
  response = s.get('http://www.zhihu.com')

  
  若要用在实际的爬虫中，需要在上述的代码做如下两方面处理:
  + 模拟浏览器的 http request header，在用 requests 请求数据时添加些类似的
    header 数据
  + 通过代理实现抓取，这样可以提高抓取频率 (可以拿一个独立 IP 尝试下知乎所能允许
    的最大抓取频率)
* 不寻常的网站
  http://www.proxytm.com/free-http-proxy-server-lists/type-anonymous-1.html
  这个网站会在第一次访问时设置 cookie，返回一个只包含一段 js 代码的页面指导跳转。
  之后每次再访问时，若加了相应的 cookie，则不再跳转. 
