* spider trap
  网站可能为了不让或限制爬虫爬资源，会设置 spider trap，常见的情况是让爬虫经过爬
  链后在一个循环链中不停地爬。
  在写爬虫时，要格外注意出现爬链死循环的情况，即爬链时出现死循环的情况。
  在我写过的爬虫中，如抓取亚马逊的 "商品id:评论id" 的爬虫，在爬"下一页"的链接过
  程中，出现了死循环的情况。因为那时我只找对应的 a[@href] 的最后一个，没有分析它
  是否对应"下一页"。
