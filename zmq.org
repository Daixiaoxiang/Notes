* Facts about ZMQ
** The Zen of Zero
   The 0 in 0MQ is all about tradeoffs.
   Originally the 0 in 0MQ was meant as 'zero broker' and (as close to) 'zero
   latency' (as possible). Since then, it has come to encompass different goals:
   0 administration, 0 cost, 0 waste. More generally, 0 refers to the culture of
   minimalism that permeats the project.
** Some other facts
   + It delivers blobs of data (messages) to nodes, quickly and efficiently.
   + It gives your applications a signle socket API to work with, no matter
     what the actual transport (like in-process, inter-process, TCP or
     multicast).
   + It automatically reconnects to peers as they come and go.
   + It queues messages at both sender and receiver, as needed.
   + It manages these queues carefully to ensure process don't run out of
     memory, overflowing to disk when appropriate.
   + It handles socket errors.
   + It does all I/O in background threads.
   + It uses lock-free techniques for talking between nodes, so there are never
     locks, waits, semaphores, or deadlocks.
   + It routes and queues messages according to precise recipes
     called *patterns*. It is these patterns that provide ZMQ's
     intelligence. They encapsulate our hard-earned experience of the best ways
     to distribute data and work.
* 几种通信场景
  + in-process
  + inter-process
  + TCP
  + multicast
* context
  You should create and use exactly one client in your process. The context is
  the container for all sockets in a single process, and acts as the transport
  for inproc sockets, which are the fastest way to connect threads in one
  process.
* 设计 messaging system 常见的问题
  + How do we handle I/O? 
    Does our application block, or do we handle I/O in the background? This is a
    key design decision. 
    Blocking I/O creates architectures that do not scale well. 
    But background I/O can be very hard to do right.
  + How do we handle dynamic components, i.e., pieces that go away temporialy?
    Do we formally split components into 'clients' and 'servers' and mandate
    that servers cannot disappear? What then if we want to connect servers to
    servers? Do we try to connect every few second?
	That is, how do pieces know about each other? This is called 
    *dynamic discovery problem*.
  + How do we represent a message on the wire? 
    How do we frame data so it's easy to write and read, safe from buffer
    overflows, efficient for small messages, yet adequate for the very largest
    videos of dancing cats wearing party hats?
  + How do we handle messages that we can't deliver immediately? 
    Particularly, if we're waiting for a component to come back online? Do we
    discard messages, put them into a database, or into a memory queue?
  + Where do we store message queues? What happens if the component reading
    from a queue is very slow and causes our queues to build up? What's our
    strategy then?
  + How do we handle lost messages? Do we wait for fresh data, request a
    resend, or do we build some kind of reliability layer that ensures messages
    cannot be lost? What if that layer itself crashes?
  + What if we need to use a different network transport. Say, multicast
    instead of TCP unicast? Or IPv6? Do we need to rewrite the applications, or
    is the transport abstracted in some layer?
  + How do we route messages? 
    Can we send the same message to multiple peers?
    Can we send replies back to an original requester?
  + How do we write an API for another language? 
    Do we re-implement a wire-level protocol or do we repackage a library? If
    the former, how can we guarantee efficient and stable stacks? If the
    latter, how can be guarantee interoperability?
  + How do we represent data so that it can be read between different
    architectures? Do we enforce a particular encoding for data types? How far
    is this the job of the messaging system rather than a higher layer?
  + How do we handle network errors? Do we wait and retry, ignore them
    silently, or abort?
* 'socket' in zmq (IMPORTANT)
** concept
   Traditional network programming is built on the general assumption that one
   socket talks to one connection, one peer. There are multicast protocols, but
   these are exotic. When we assume 'one socket = one connection', we scale our
   architecture in certain ways. We create threads of logic where each thread
   work with one socket, one peer. We place intelligence and state in these
   threads.
   
   In the ZMQ universe, sockets are doorways to fast little background
   communications engines that manage a whole set of connections automatically
   for you. You can't see, work with, open, close, or attach state to these
   connections. Whether you use blocking send or receive, or poll, all you can
   talk to is the socket, not the connections it manages for you. The
   connections are private and invisible, and this is the key to ZMQ's
   scalability.
   
   This is because your code, talking to a socket, can then handle any number of
   connections across network protocols are around, without change. A messaging
   pattern sitting in ZMQ scales more cheaply than a messaging pattern sitting
   in your application code.
** lifecycle of socket in zmq
   Sockets have a life in four parts, just like BSD sockets:
   + Creating and destroying sockets, which go together to form a karmic circle
     of socket life.
   + Configuring socket by setting options on them and checking them if
     necessary.
   + Plugging sockets into the network topology by creating ZMQ connections to
     and from them.
   + Using the sockets to carry data by writing and receiving messages on them.
** 'bind' in ZMQ
   When a socket is bound to an endpoint it automatically starts accepting
   connections. 
** 'connection' in ZMQ
   The network connection itself happens in the background, and ZMQ will
   automatically reconnect if the network connection is broken (e.g. if the peer
   disappears and then comes back).

   Your application code can't work with these connections directly; they are
   encapsulated under the socket.
** socket types
   Sockets have types. The socket type defines the semantics of the socket, its
   policies for routing messages inwards and outwards, queuing, etc.

   It's the ability to connect sockets in different ways that gives ZMQ its
   basic power as a message queuing system.
* Transports
** summary
   ZMQ provides a set of *unicast* transports (_inproc_, _ipc_ and _tcp_)
   and *multicast* transports (_epgm_, _pgm_).

   For most common cases, use *tcp*, which is a disconnected TCP transport. We
   call it disconnected because ZMQ's *tcp* transport doesn't require that the
   endpoint exists before you connect to it. Clients and severs can connect and
   bind at any time, can go and come back, and it remains transparent to
   applications. 

   The inter-process *ipc* transport is disconnected, like *tcp*. It has one
   limitation: it does not yet work on Windows. By convention, we use endpoint
   names with an *.ipc* extension to avoid potential conflict with other file
   names. On Unix systems, if you use *ipc* endpoints, you need to  create
   these with appropriate permissions otherwise they may not be shareable
   between processes running under differenct user IDs. You must also make sure
   all processes can access the files, e.g., by running in the same working
   directory. 

   The inter-thread transport, *inproc*, is a connected signaling transport. It
   is much faster than *tcp* or *ipc*. This transport has a specific limitation
   compared to *tcp* and *icp*: the server must issue a bind before any client
   issues a connect. We create and bind one socket and start the child threads,
   which create and connect the other sockets.
* Patterns (IMPORTANT)
** summary
   ZMQ patterns are implemented by pairs of sockets with matching types.
   
   The built-in core ZMQ patterns are:
   + *Requets-Reply*
	 It connects a set of clients to a set of services. 
     _This is a remote procedure call and task distribution pattern._
   + *Pub-Sub*
	 It connects a set of publishers to a set of subscribers.
	 _This is a data distribution pattern_
   + *Pipeline* (PUSH/PULL)
	 It connects nodes in a fan-out/fan-in pattern that can have multiple steps
     and loops.
	 _This is a parallel task distribution and collection pattern_
   + *Exclusive Pair*
	 It connects two sockets exclusively.
	 _This is a pattern for connecting two threads in a process, not to be_
     _confused with 'normal' pairs of sockets._

  The *zmp_socket()* man page is fairly clear about the patterns -- it's worth
  reading several times until it starts to make sense.
** socket combinatinos
   These are the socket combinations that are valid for a connection-bind pair
   (either side can bind):
   + PUB/SUB
   + REQ/REP
	 两方只用发送信息的具体的内容即可. socket 处理时会自动把该信息封装成一个
     frame，然后在该 frame 之前添加或除去一个空的 frame.
   + REQ/ROUTER
	 - 若 REQ socket 没有设置 zmq.IDENTITY
	   则 REQ 发送信息时只用发送信息实体，该 socket 会自动把信息封装成一个 frame，
       然后在发送该 frame 之前发送一个空的 frame。接收 ROUTER 发来的信息时，REQ
       socket 会收到两个 frame，第一个 frame 是空的，会被 REQ socket 自动除去，
       只将第二个包含信息的 frame 交给应用.
	   ROUTER socket 会自动生成一个该 REQ socket 的唯一标志符，处理后会在接收到
       的两个 frame 之前添加一个唯一标志符的 frame，将这三个 frame 交给 ROUTER
       处理.向 REQ socket 发送消息时也需要按此发送三个 frame，只是 ROUTER socket
       会在定位接收信息的 REQ socket 后自动除去第一个唯一标志的 frame，这样 REQ
       socket 实际接收到的是两个 frame，第一个 frame 是空，第二个 frame 是信息实体.
	 - 若 REQ socket 设置了 zmq.IDENTITY
	   大致的处理与上类似，只是会以该 IDENTITY 为该 connection 的唯一标志.
   + DEALER/REP
   + DEALER/ROUTER
   + DEALER/DEALER
   + ROUTER/ROUTER
   + PUSH/PULL
   + PAIR/PAIR
   + XPUB/XSUB
	 XPUB/XSUB are exactly like PUB/SUB except that they expose subscriptions
     as special messages. 
     The proxy has to forward these subscription messages from subscriber side
     to publisher side, by reading them from the XSUB socket and writing them
     to the XPUB socket.
	 This is the main use case for XPUB/XSUB.
** Pub-Sub
*** Envelope
	In the pub-sub pattern, we can split the key into a separate message frame
	that we call an *envelope*. Using an envelope could delimit keys from data
	so that the prefix match doesn't accidently match data and the match won't
	cross a frame boundary.

	The corresponding API is:
	+ send_multipart([FRAME0, FRAME1, FRAME2, ...])
	+ recv_multipart()
	  返回 [FRAME0, FRAME1, FRAME2, ...]
** 发送和接收信息
*** REQ
	+ REQ/ROUTER
	  REQ socket 会接收到两个 frame，第一个 frame 是空，第二个 frame 是信息实体.它
	  会自动除去第一个 frame，然后将第二个 frame 交给 application.
   	  application 只发送信息实体，REQ socket 会生成两个 frame (???这点暂时有疑问)，
	  第一个是空的 frame，第二个是包含信息实体的 frame，然后将该信息发送给 ROUTER.

	REQ socket 的工作形式：发送消息-->接收消息-->发送消息-->接收消息-->...
	在接收消息之前必须先发送消息.
*** ROUTER
	+ ROUTER/REQ
	  ROUTER application 会收到三个 frame，第一个 frame 是发送方的地址(唯一标志符)，
	  第二个 frame 是空，第三个 frame 是信息实体.
	  ROUTER application 发送消息时，也需要发送上述三个 frame.
	+ ROUTER/DEALER
	  ROUTER application 会收到两个 frame，第一个 frame 是发送方的地址(唯一标志
      符)，第二个 frame 是信息实体.
	  ROUTER application 发送消息时，也需要发送上述两个 frame. ROUTER socket 会
      根据第一个 frame 确认要发送给哪个 DEALER socket，然后将第二个 frame 发送
      给 DEALER socket.

   ROUTER socket 的工作形式: 发送/接收消息的顺序可任意,也可只接收不发送或只发送
   不接收.
*** DEALER
	+ DEALER/ROUTER
	  DEALER application 发送消息时只用发送信息实体.
	  DEALER socket 和 application 接收消息时也只接收到信息实体.

   DEALER socket 的工作形式: 发送/接收消息的顺序可任意，也可只接收不发送或只发送
   不接收.
* Messages/Frames (IMPORTANT)
** A useful lexicon
   + A message can be one or more parts.
   + These parts are also called 'frames'.
   + Each part is a *zmq_msg_t* object.
   + You send and receive each part separately, in the low-level API.
   + Higher-level APIs provide wrappers to send entire multipart messages.
** Some things worth knowing about messages:
   + You may send zero-length messages, e.g., for sending a signal from one
     thread to another
   + ZMQ guarantees to deliver all the parts (one or more) for a message, or
     none of them
   + ZMQ does not send the message (single or multipart) right away, but at
     some indeterminate time. A multipart message must therefore fit in memory
   + A message (single or multipart) must fit in memory. If you want to send
     files of arbitrary sizes, you should break them into pieces and send each
     piece as separate single-part messages. Using multipart data will not
     reduce memory consumption
   + You must call *zmq_msg_close()* when finished with a received message, in
     languages that don't automatically destroy objects when a scope
     closes. You don't call this method after sending a message.
* Handling multiple sockets
  To actually read from multiple sockets all at once, use *zmq_poll()*.
* Multipart Messages
** Some things to know about multipart messages
   + When you send a multipart message, the first part (and all following
     parts) are only actually sent on the wire when you send the final part.
   + If you are using *zmq_poll()*, when you recieve the first part of a
     message, all the rest has also arrived.
   + You will receive all parts of a message, or none at all.
   + Each part of a message is a separate *zmq_msg* item.
   + You will receive all parts of a message whether or not you check the more
     property.
   + On sending, ZMQ queues message frames in memory until the last is
     received, then sends them all.
   + There is no way to cancel a partially sent message, except by closing the
     socket. 
* Multithreads in ZMQ
** Some rules
   + Isolate data privately within its thread and never share data in multiple
     threads. The only exception to this are ZMQ contexts, which are threadsafe.
   + Stay away from the classic concurrency mechanisms like as mutexes,
     critical sections, semaphores, etc. These are an anti-pattern in ZMQ
     applications.
   + Create one ZMQ context at the start of your process, and pass that to all
     threads that you want to connect via *inproc* sockets.
   + Use _attached_ threads to create structure within your application, and
     connect these to their parent threads using *PAIR* sockets
     over *inproc*. The pattern is: bind parent socket, then create child
     thread which connects its socket.
   + Use _detached_ threads to simulate independent tasks, with their own
     contexts. Connect these over *tcp*. Later you can move these to
     stand-alone processes without changing the code significally.
   + All interaction between threads happens as ZMQ messages, which you can
     define more or less formally.
   + Don't share ZMQ sockets between threads. ZMQ sockets are not
     threadsafe. Technically it's possible to migrate a socket from one thread
     to another but it demands skill. The only place where it's remotely sane
     to share sockets between threads are in language bindings that need to do
     magic like garbage collection on sockets.
   + Do not use or close sockets except in the thread that created them.
** Coordination between pairs of threads
   The best practice is to use *zmq.PAIR*. Here's the reason:
   + You can use *PUSH* for the sender and *PULL* for the receiver. This looks
     simple and will work, but remeber that *PUSH* will distribute messages to
     all available receivers. If you by accident start two receivers (e.g., you
     already have one running and you start a second), you'll "lose" half of
     your signals. *PAIR* has the advantage of refusing more than one
     connection; the pair is exclusive.
   + You can use *DEALER* for the sender and *ROUTER* for the
     receiver. *ROUTER*, however, wraps your message in an "envelope", meaning
     your zero-size signal turns into a multipart message. If you don't care
     about the data and treat anything as a valid signal, and if you don't read
     more than once from the socket, that won't matter. If, however, you decide
     to send real data, you will suddenly find *ROUTER* providing you with
     "wrong" messages. *DEALER* also distribute outgoing messages, giving the
     same risk as *PUSH*
   + You can use *PUB* for the sender and *SUB* for the receiver. This will
     correctly deliver your messages exactly as you sent them and *PUB* does
     not distribute as *PUSH* or *DEALER* do. However, you need to configure
     the subscriber with an empty subscription, which is annoying.
** How to interrupt child workes?
   If you're using child threads ,they won't receive the interrupt. To tell
   them to shutdown, you can either:
   + Destroy the context, if they are sharing the same context, in which case
     any blocking calls they are waiting on will end with *ETREM*
   + Send them shutdown messages, if they are using their own contexts. For
     this you'll need some socket plumbing.
* High-Water Marks (IMPORTANT)
  它解决的是消息 发送/接收 两方速度不匹配的问题，即 *flow-control* 问题.通过
  + 丢掉消息
  + 阻塞发送方
  两种方法来解决。一般有 *发送* 和 *接收* 两种类型的 HWM.
  对于不同的 pattern，对过量的消息有不同的处理方法:
  + PUB and ROUTER sockets will drop data if they reach their HWM
  + Other socket types will block.
* Missing Message Problem Solver (IMPORTANT)
  参考这篇文档中的图:
  + [[http://zguide.zeromq.org/page:all#Missing-Message-Problem-Solver][Missing-Message-Problem-Solver]]

  解释:
  + On *SUB* sockets, set a subscription using *zmq_setsockopt()*
    with *ZMQ_SUBSCRIBE*, or you won't get messages. Because you subscribe to
    messages by prefix, if you subscribe to '' (an empty subscription), you
    will get everything.
  + If you start the *SUB* socket (i.e., establish a connection to a *PUB*
    socket) after the *PUB* socket has started sending out data, you will lose
    whatever it published before the connection was made. If this is a problem,
    set up your architecture so the *SUB* socket starts first, then the *PUB*
    socket starts publishing.
  + [???] Even if you synchronize a *SUB* and *PUB* socket, you may still lose
    message. It's due to the fact that internal queues aren't created until a
    connection is actually created. If you can switch the bind/connect
    direction so the *SUB* socket binds, and the *PUB* socket connects, you may
    find it works more as you'd expect.
  + If you're using *REP* and *REQ* sockets, and you're not sticking to the
    synchronous send/recv/send/recv order, ZMQ will report errors, which you
    might ignore. Then it would look like you're losing messages. If you
    use *REQ* or *REP*, stick to the send/recv order, and always, in real code,
    check for errors on ZMQ calls.
  + If you're using *PUSH* sockets, you'll find that the first *PULL* socket to
    connect will grab an unfair share of messages. The accurate rotation of
    messages only happens when all *PULL* sockets are successfully connected,
    which can take some milliseconds. As an alternative to *PUSH/PULL*, for
    lower data rates, consider using *ROUTER/DEALER* and the load balancing
    pattern.
  + If you're sharing sockets across threads, don't. It will lead to random
    weirdness, and crashes.
  + If you're using *inproc*, make sure both sockets are in the same
    context. Otherwise the connectin side will in fact fail. Also, bind first,
    then connect. *inproc* is not a disconnected transport like *tcp*.
  + If you're using *ROUTER* sockets, it's remarkably easy to lose messages by
    accident, by sending malformed identity frames (or forgetting to send an
    identity frame). In general setting the *ZMQ_ROUTER_MANDATORY* option
    on *ROUTER* sockets is a good idea, but also check the return code on every
    send call.
  + Lastly, if you really can't figure out what's going wrong, make a minimal
    test case that reproduces the problem, and ask for help from the ZMQ
    community.
* REQ/REP Patterns
** Some things to know
   + The *REQ* socket sends, to the network, an empty delimiter frame in front
     of the message data. 
     *REQ* sockets are _synchronous_. *REQ* sockets always send one request and
     then wait for one reply.
	 *REQ* sockets talks to one peer at a time. If you connect a *REQ* socket
     to multiple peers, requests are distributed to and replies expected from
     each peer one turn at a time.
   + The *REP* socket reads and saves all identity frames up to and including
     the empty delimiter, then passe the following frame or frames to the
     caller.
	 *REP* sockets are _synchronous_ and talk to one peer at a time. If you
     connect a *REP* socket to multiple peers, requests are read from peers in
     fair fashion, and replies are always sent to the same peer that made the
     last request.
   + The *DEALER* socket is oblivious to the reply envelope and handles this
     like any multipart message.
	 *DEALER* sockets are _asynchronous_ and like *PUSH* and *PULL*
     combined. They distribute sent messages among all connections, and
     fair-queue received messages from all connections.
   + The *ROUTER* socket is obliviious to the reply envelope, like *DEALER*. It
     creates identities for its connections, and passes these identities to the
     caller as a first frame in any received message. Conversely, when the
     caller sends a message, it uses the first message frame as an identity to
     look up the connection to send to.
	 *ROUTERS* are _asynchronous_.
** req-rep combinatinos
   These are the legal combinations:
   + REQ to REP
   + DEALER to REP
   + REQ to ROUTER
   + DEALER to ROUTER
   + DEALER to DEALER
   + ROUTER to ROUTER

   There combinations are invalid:
   + REQ to REQ
   + REQ to DEALER
   + REP to REP
   + REP to ROUTER

   Here're some tips for remembering the semantics.
   *DEALER* is like an _asynchronous_ *REQ* socket, and *ROUTER* is like an
   _asynchronous_ *REP* socket. Where we use a *REQ* socket, we can use
   a *DEALER*; we just have to read and write the envelope ourselves. Where we
   use a *REP* socket, we can stick a *ROUTER*; we just need to manage the
   identities ourselves.

   Think of *REQ* and *DEALER* sockets as "clients" and *REP* and *ROUTER*
   sockets as "servers".*
** ways to connect clients to servers
   There're roughly three ways to connect clients to servers. Each needs a
   specific approach to reliability:
   + Multiple clients talking directly to a single server.
	 - Use case
       a single well-known server to which clients need to talk.
	 - Types of failure
	   server crashes and restarts, and network disconnects.
   + Multiple clients talking to a broker proxy that distributes work to
     multiple workers.
	 - User case
	   service-oriented transaction processing.
	 - Types of failure
	   worker crashes and restarts, worker busy looping, worker overload, queue
       crashes and restarts, and network disconnects.
   + Multiple clients talking to multiple servers with no intermediary proxies.
	 - Use case
	   distributed services such as name resolution
	 - Types of failure
	   service crashes and restarts, service busy looping, service overload,
       and network disconnects.
** reliability
*** client-side reliabity (lazy pirate pattern)
	Rather than doing a blocking receive, we:
	+ Poll the REQ socket and receive from it only when it's sure a reply has
      arrived.
	+ Resend a request, if no reply has arrived within a timeout period.
	+ Abandon the transaction if there is still no reply after several requests.

	If you try to use a REQ socket in anything other than a strict send/receive
	fashion, you'll get an error (technically, the REQ socket implement a small
	finite-state machine to enforce the send/receive ping-pong, and so the error
	is called "EFSM").

	The pretty good brute force solution is to close and reopen the REQ socket
	after an error.

	Pros and cons:
	+ Pros
	  - simple to understand and implement
	  - works easily with existing client and server application code
	  - ZMQ automatically retries the actual reconnection until it works
	+ Cons
	  - doesn't failover to backup or alternate servers.
* Difference between REQ and DEALER
   Anywhere you can use *REQ*, you can use *DEALER*. There are two specific
   difference:
   + The *REQ* socket always sends an empty delimiter frame before any data
     frames; the *DEALER* does not.
   + The *REQ* socket will send only one message before it receives a reply;
     the *DEALER* is fully asynchrounous.
* I/O
** summary
   ZMQ does I/O in a background thread. One I/O thread (for all sockets) is
   sufficient for all but the most extreme applications. When you create a new
   context, it starts with one I/O thread. The general rule of thumb is to
   allow one I/O thread per gigabyte of data in or out per second. But you can
   change the number of I/O threads.
* FAQ
** Does 'zmq_send()' method actually send messages?
   No. It queues the messages so that the I/O thread can send it
   asynchrounously. It does not block except in some exception cases. So the
   message is not necessarily sent when *zmq_send()* returns to your
   application.
** the natural patterns that 'bind'/'connect'
   The side which we expect to "be there" *binds*: it'll be 
   + a server
   + a broker
   + a publisher
   + a collector.

   The side that "comes and goes" *connects*: it'll be 
   + clients
   + workers
** 需要多重复看哪些章节?
   与 identity 相关的:
   + [[http://zguide.zeromq.org/page:all#The-Simple-Reply-Envelope][The Simple Reply Envelope]]
   + [[http://zguide.zeromq.org/page:all#The-Extended-Reply-Envelope][The Extended Reply Envelope]]
   + [[http://zguide.zeromq.org/page:all#Identities-and-Addresses][Identities and Addresses]]

   Request-Reply Combinations:
   + [[http://zguide.zeromq.org/page:all#Request-Reply-Combinations][Request-Reply Combinations]]
** send()/recv() 发送/接收 的是什么？
   是一个 frame.
   
** send_multipart/recv_multipart() 发送/接收 的是什么?
   addr_worker = router.recv()
   empty = router.recv()
   body = router.recv()
   等同于:
   addr_worker, empty, body = router.recv_multipart()

   router.send(addr_worker, zmq.SNDMORE)
   router.send('', zmq.SNDMORE)
   router.send('END')
   等同于:
   router.send_multipart([addr_worker, '', 'END'])
* Reliability
** Some probabilities:
   + Application code is the worse offender. It can crash and exit, freeze, and
     stop responding to input, run too slowly for its input, exhaust all
     momeory, and so on.
   + System code -- such as brokers written using ZMQ -- can die for same
     reasons as application code. System code should be more reliable than
     application code, but it can still crash and burn, and especially run out
     of memory if it tries to queue messages for slow clients.
   + Message queues can overflow, typically in system code that has learned to
     deal brutally with slow clients. When a queue overflow, it starts to
     discard messages.
   + Networks can fail (e.g., WiFi gets switched off or goes out of range). ZMQ
     will automatically reconnect in such cases, but in the meantime, messages
     may get lost.
   + Hardware can fail and take with it all the processes running on that box.
   + Networks can fail in exotic ways, e.g., some ports on a switch may die and
     those parts of the network become inaccessible.
   + Entire data centers can be struck by lightening, earthquakes, fire, or
     more mundane power or cooling failure.
** Some patterns
   + Request-reply
	 If the server dies (while processing a request), the client can figure
     that out because it won't get an answer back. Then it can give up, wait
     and try again later, find another server, and so on. 
     As for the client dying, we can brush that off as "someone else's problem"
     for now. 
   + Pub-sub
	 If the client dies (having gotten some data), the server doesn't know
     about it. Pub-sub doesn't send any information back from client to
     server. But the client can contact the server out-of-band, e.g., via
     request-reply, and ask, "please resend everything I missed".
	 Subscribers can also self-verify that they're not running too slowly, and
     take action (e.g., warn the operator and die) if they are.
   + Pipeline
	 If a worker dies (while working), the ventilator doesn't know about
     it. Pipelines, liek the grinding gears of time, only work in one
     direction. But the downstream collector can detect that one task didn't
     get done, and send a message back to the ventilator saying, "hey, resend
     task 324!".
	 If the ventilator or collector dies, whatever upstream client originally
     sent the work batch can get tired of waiting and resend the whole
     lot. It's not elegant, but system code should really not die oftern enough
     to matter.
* BEST PRACTICE
** 建立连接时设立同步机制
   由于 zmq 发送消息很快，可能在多个 client 建立连接过程中就已经把消息发送完，而
   此时 client 因并未建立好连接造成没有接收到消息，故需要一定的同步机制.

   最简单的同步设立机制是在 client 启动后让 server 等待一段时间后再发送消息.
** send()/recv() 和 send_multipart()/recv_multipart() 的选择
   要根据是使用场景进行选择:
   + 未知消息中 frame 个数时
	 最好选择 send()/recv()
   + 已知消息中 frame 个数时
	 可选择 send_multipart()/recv_multipart()


* message patterns
** 概述
   In distributed architecture, different parts of system interconnect and
   communicate with each other. These interconnecting systems viewed graphically
   represents a network topology.

   Messaging patterns are network oriented architectural pattern that describes
   the flow of communication between interconnecting systems. ØMQ provides
   pre-optimized sockets which enables you to take advantage of these patterns.

   Each pattern in ØMQ defines the constraints on the network topology. What
   systems can connect to each other and flow of communication between
   them. These patterns are designed to scale.
** PAIR
   It provides sockets that are close in behavior to conventional sockets.

   Conventional sockets allow:
   + only strict one-to-one (two peers)
   + many-to-one (many clients, one server)
   + one-to-many (multicast) relationships

   Paired sockets are very similar to regular sockets:
   + The communication is bidirectional.
   + There is no specific state stored within the socket.
   + There can only be one connected peer.
   + The server listens on a certain port and a client connects to it.

   采用的 pattern 都是 zmq.PAIR.
** Client/Server
   它是 "多 clients-->一个 server" 的形式，其中每个 'client-->server' 类似于
   PAIR pattern，区别在于：
   + ZMQ REQ sockets can connect to many servers.
   + The requests will be interleaved or distributed to both the servers.
   + socket zmq.REQ will block on send unless it has successfully received a
     reply back.
   + socket zmq.REP will block on recv unless it has received a request.

   Each Request/Reply is paired and has to be successful.
** Publisher/Subscriber
   Publish/Subscribe is another classic pattern where senders of messages,
   called publishers, do not program the messages to be sent directly to
   specific receivers, called subscribers. Messages are published without the
   knowledge of what or if any subscriber of that knowledge exists.

   The pattern is intended for event and data distribution, usually from a small
   number of publishers to a large number of subscribers, but also from many
   publishers to a few subscribers. For many-to-many use-cases the pattern
   provides raw socket types (XPUB, XSUB) to construct distribution proxies,
   also called brokers.
** Push/Pull
   Push and Pull sockets let you distribute messages to multiple workers,
   arranged in a pipeline. A Push socket will distribute sent messages to its
   Pull clients evenly. This is equivalent to producer/consumer model but the
   results computed by consumer are not sent upstream but downstream to another
   pull/consumer socket.

   Data always flows down the pipeline, and each stage of the pipeline is
   connected to at least one node. When a pipeline stage is connected to
   multiple nodes data is load-balanced among all connected nodes.

   The pattern is intended for task distribution, typically in a multi-stage
   pipeline where one or a few nodes push work to many workers, and they in turn
   push results to one or a few collectors. The pattern is mostly reliable
   insofar as it will not discard messages unless a node disconnects
   unexpectedly. It is scalable in that nodes can join at any time.

   + push socket 如何管理消息
     - 只发送消息 
	 - 当没有可用的 pull socket 时, push socket 在 send 时阻塞
	 - 'pull socket 是否可用' 是通过相应的 outgoing queue 是否满来判断
	 - 对于每个 pull socket 的连接，都会建立并维护一个 outgoing queue 来管理 push
       socket 向 pull socket 发送的信息 
	 - 每个 outgoing queue 的容量有限
	 - 当 pull socket 断开连接后，相应的 outgoing queue 中缓冲的消息会被丢弃
	 - 对于没有放到 outgoing queue 中的消息, push socket 不能丢弃
	 - 分发消息时，是 round-robin 的形式，依次向可用的 pull socket 发送消息 
   + pull socket 如何管理消息
	 - 只接收消息
	 - 对 push socket 的连接，都会建立并维护一个的 incoming queue 管理 push socket
       发送的消息 (不管连接是否建立成功都会维护一个 incoming queue)
	 - 若对应的 push socket 失去连接时，该 incoming queue 会被销毁，其中的信息会被
       丢弃
	 - 每个 incoming queue 的容量有限
	 - 通过 fair-queuing 的形式接收消息，类似于 round-robin 形式 
* 多 socket 问题
  常会有多个同类的 socket 和另一种 socket 进行交互，这时需要 poller 对数据进行轮
  混处理，否则用 socket 的 recv() 方法时会阻塞当前的进程.
* poller 解决的问题
  主要解决多个 socket 在同一个逻辑块内时，防止调用任一个 socket 的 recv() 方法时
  阻塞整个进程的问题. 
* 信息的流动过程
  socket-->outgoing queue-->kernel buffer-->wire-->kernel buffer-->incoming
  queue-->socket
